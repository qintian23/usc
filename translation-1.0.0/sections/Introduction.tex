% !TeX root = ../translation.tex

\newpage

\section{介绍}

生成对抗网络(GANs)已成为无条件图像生成的主要方法之一。当在多个数据集上进行训练时，GANs能够产生真实和视觉上吸引人的样本。GAN方法训练了一个无条件的生成器，从随机噪声中回归真实图像，以及一个鉴别器，测量生成的样本和真实图像之间的差异。GANs已经得到了各种改进。生成对抗网络(GANs)已成为无条件图像生成的主要方法之一。当在多个数据集上进行训练时，GANs能够产生真实和视觉上吸引人的样本。GAN方法训练了一个无条件的生成器，从随机噪声中回归真实图像，以及一个鉴别器，测量生成的样本和真实图像之间的差异。GANs已经得到了各种改进。

\subsection{流形分布假说}

差距的巨大成功可以解释为差距有效地发现真实数据集的内在结构，可以制定为流形分布假设：一个特定类的自然数据集中在低维流形嵌入在高维背景空间[2]。

图1显示了MNIST数据库的流形结构。每个手写数字图像具有尺寸28×28，并被视为图像空间中的一个点。MNIST数据库集中在一个低维流形附近。通过使用t-SNE流形嵌入算法[3]，MNIST数据库被映射到一个平面域上，每个图像被映射到一个点上。代表相同数字的图像被映射到一个簇上，10个簇被颜色编码。这表明MNIST数据库分布在嵌入单元立方体中的二维（2D）曲面附近。

\subsection{GANs的理论模型}

图2为GANs的理论模型。真实的数据分布v集中在嵌入在环境空间χ中的流形Σ上。(Σ，v)一起显示了真实数据集的内在结构。一个GAN模型计算一个从潜在空间Z到流形Σ的生成器映射gθ，其中θ表示一个深度神经网络(DNN)的参数。ζ是潜在空间中的高斯分布，gθ将ζ向前推到µθ。鉴别器计算真实数据分布v和生成的分布µθ之间的距离，如瓦瑟斯坦距离Wc(µθ，v)，它相当于康塔罗维奇的潜在φξ

尽管GANs有优势，但它们也有严重的缺点。在理论上，对深度学习的基本原则的理解仍然是原始的。在实践中，GANs的训练是棘手的，对超参数是敏感的；GANs遭受模式崩溃。最近，Meschede等人。[4]研究了9种不同的GAN模型和变体，表明基于梯度下降的GAN优化并不总是局部收敛的。

尽管GANs有优势，但它们也有严重的缺点。在理论上，对深度学习的基本原则的理解仍然是原始的。在实践中，GANs的训练是棘手的，对超参数是敏感的；GANs遭受模式崩溃。最近，Meschede等人。[4]研究了9种不同的GAN模型和变体，表明基于梯度下降的GAN优化并不总是局部收敛的。

图3为生成器图gθ=ℎ∘T的分解，其中为从潜在空间到h：Z环境空间中的数据流形Σ，概率分布变换图T：Z→Z。解码图ℎ用于流形学习，图T用于测量传输。

\subsection{最优传输的观点}

OT理论[5]研究了以最经济的方式将一种概率分布转化为另一种概率分布的问题。OT提供了严格而强大的方法来计算最优映射，将一个概率分布转换为另一个分布，并确定它们之间的距离为[6]。

如前所述，GANs完成了两个主要任务：流形学习和概率分布变换。后一种任务可以直接通过OT方法完全完成。具体地说，在图3中，可以用OT理论计算出概率分布变换图T。该鉴别器计算生成的数据分布与真实数据分布之间的瓦瑟斯坦距离Wc(µθ，v)，可以用OT方法直接计算。

从理论的角度来看，天然气的解释使黑盒子透明的一部分，概率分布变换减少凸优化过程使用OT理论，解决方案的存在性和唯一性理论保证，收敛速度和近似精度充分分析。

OT的解释也解释了模态崩溃的根本原因。根据蒙格-安培方程的正则性理论，运输映射在某些奇异集合上是不连续的。然而，DNN只能建模连续的函数/映射。因此，目标交通映射是在GANs可表示的功能空间之外的。这种内在的冲突使模式的崩溃不可避免。

OT解释也揭示了发生器和鉴别器之间更为复杂的关系。在当前的GAN模型中，生成器和鉴别器相互竞争，而不共享中间的计算结果。OT理论表明，在l2代价函数下，生成器的最优解和鉴别器的最优解可以用封闭的形式相互表示。因此，应该用协作来代替发生器和鉴别器之间的竞争，并应该共享中间的计算结果，以提高效率。

\subsection{自动编码器-最优运输模型}

为了减少GANs的训练难度，特别是为了避免模式崩溃，我们提出了一个基于OT理论的更简单的生成模型：一个自动编码器(AE)-OT模型，如图4所示。

如前所述，生成模型的两个主要任务是流形学习和概率分布变换。AE计算编码映射，fθ：Z→Σ，和解码映射gξ：Σ→Z，用于流形学习。OT图，T：Z→Z，将白噪声ζ转换为由编码图推送转发的数据分布，(fθ)\#v。

AE-OT模型有很多优点。从理论的角度来看，OT理论已经得到了很好的建立和充分的理解。通过对解码图和OT图进行解耦，可以提高生成模型的理论严谨性，使黑盒的一部分透明。在实际应用中，将OT映射简化为一个凸优化问题，保证了解的存在性和唯一性，使训练过程不会陷入局部最优状态。与OT映射相关的凸能量具有显式的黑森矩阵；因此，优化可以采用二阶收敛的牛顿方法，或采用超线性收敛的拟牛顿方法进行优化。相比之下，目前的生成模型基于线性收敛的梯度下降法；未知数等于训练样本，以避免过参数化问题；蒙特卡罗方法中的采样密度可以完全控制OT映射的误差界；具有自适应的层次算法进一步提高了效率；并行OT映射算法可以使用图形处理单元(GPU)实现。最重要的是，AE-OT模型可以消除模式崩溃。

\subsection{贡献}

本工作利用OT理论解释了GAN模型。GANs可以完成两个主要任务：流形学习和概率分布变换。后一种任务可以使用OT方法来完成。生成器计算一个OT图，而鉴别器计算生成的数据分布和真实数据分布之间的瓦瑟斯坦距离。利用布雷尼尔的理论，发电机和鉴别器之间的竞争可以被协作所取代；根据蒙格-安培方程的正则性理论，运输图的不连续导致了模态崩溃。我们进一步提出了利用AE-OT模型来解耦流形学习和概率分布变换，使部分黑箱透明，提高了训练效率，防止了模式崩溃。实验结果证明了该方法的有效性和有效性。

本文的组织结构如下：第2节简要回顾了OT和GANs中最相关的工作；第3节简要介绍了OT的基本理论和蒙格-安培方程的正则性理论；第4节介绍了计算OT的变分框架；第5节从OT的角度分析了发生器与鉴别器之间的协作（而不是竞争）关系，揭示了模式崩溃的内在原因；第6节报告了实验结果；论文在第7节中总结。